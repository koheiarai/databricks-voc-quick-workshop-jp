# Databricks notebook source
# MAGIC %md
# MAGIC # Databricksä¸Šã®MLFlowã§`/plamo-embedding-1b`ãƒ¢ãƒ‡ãƒ«ã‚’ç®¡ç†ã™ã‚‹
# MAGIC
# MAGIC ã“ã®ä¾‹ã§ã¯ã€[pfnet/plamo-embedding-1b](https://huggingface.co/pfnet/plamo-embedding-1b)ã‚’  MLFLow ã«ãƒ­ã‚®ãƒ³ã‚°ã—ã€Unity Catalog ã§ãƒ¢ãƒ‡ãƒ«ã‚’ç®¡ç†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚
# MAGIC
# MAGIC ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ç’°å¢ƒ
# MAGIC - ãƒ©ãƒ³ã‚¿ã‚¤ãƒ : 16.3 ML Runtime
# MAGIC - ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: AWS ã® `i3.xlarge` ã¾ãŸã¯ Azure ã® `Standard_D4DS_v5` 
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ## ãƒ¢ãƒ‡ãƒ«ã‚’MLFlowã«è¨˜éŒ²ã™ã‚‹

# COMMAND ----------

import torch
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer

# You can download models from the Hugging Face Hub ğŸ¤— as follows:
tokenizer = AutoTokenizer.from_pretrained("pfnet/plamo-embedding-1b", trust_remote_code=True)
model = AutoModel.from_pretrained("pfnet/plamo-embedding-1b", trust_remote_code=True)

# COMMAND ----------

# MAGIC %md
# MAGIC # READMEé€šã‚Šã«è©¦ã™

# COMMAND ----------


device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

query = "PLaMo-Embedding-1Bã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
documents = [
    "PLaMo-Embedding-1Bã¯ã€Preferred Networks, Inc. ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚",
    "æœ€è¿‘ã¯éšåˆ†ã¨æš–ã‹ããªã‚Šã¾ã—ãŸã­ã€‚"
]

with torch.inference_mode():
    # For embedding query texts in information retrieval, please use the `encode_query` method.
    # You also need to pass the `tokenizer`.
    query_embedding = model.encode_query(query, tokenizer)
    # For other texts/sentences, please use the `encode_document` method.
    # Also, for applications other than information retrieval, please use the `encode_document` method.
    document_embeddings = model.encode_document(documents, tokenizer)


similarities = F.cosine_similarity(query_embedding, document_embeddings)
print(similarities)

# COMMAND ----------

# ã‚«ã‚¿ãƒ­ã‚°ã€ã‚¹ã‚­ãƒ¼ãƒã€ãƒ¢ãƒ‡ãƒ«åã®å®šç¾©
model_uc_catalog = "kohei_arai" #TODO: Change
model_uc_schema = "demo"
uc_model_name = "plamo_embedding_1b"

# COMMAND ----------

import mlflow
from transformers import pipeline
import numpy as np

transformers_model = {"model": model, "tokenizer": tokenizer}
task = "llm/v1/embeddings"  # correct task for embeddings

# Set the registry URI to Unity Catalog
mlflow.set_registry_uri("databricks-uc")

# Create a pipeline
embedding_pipeline = pipeline(task="feature-extraction", model=model, tokenizer=tokenizer)

# Register the model
with mlflow.start_run():
    model_info = mlflow.transformers.log_model(
        transformers_model=embedding_pipeline,
        task=task,
        artifact_path="model",
        registered_model_name=f"{model_uc_catalog}.{model_uc_schema}.{uc_model_name}"
    )


# COMMAND ----------

# MAGIC %md
# MAGIC ## ãƒ¢ãƒ‡ãƒ«ã‚’ Unity Catalog ã«ç™»éŒ²ã™ã‚‹
# MAGIC  ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€MLflowã¯Databricksãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ã—ã¾ã™ã€‚ä»£ã‚ã‚Šã«Unity Catalogã«ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ã™ã‚‹ã«ã¯ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html)ã«å¾“ã„ã€ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚µãƒ¼ãƒãƒ¼ã‚’Databricks Unity Catalogã«è¨­å®šã—ã¾ã™ã€‚
# MAGIC
# MAGIC  Unity Catalogã«ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ã™ã‚‹ã«ã¯ã€ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã§Unity CatalogãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹ãªã©ã€[ã„ãã¤ã‹ã®è¦ä»¶](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html#requirements)ãŒã‚ã‚Šã¾ã™ã€‚
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ## Unityã‚«ã‚¿ãƒ­ã‚°ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€

# COMMAND ----------

import mlflow
from mlflow import MlflowClient

# Define the registered model name and alias
registered_name = "kohei_arai.models.plamo_embedding_1b"
alias = "latest_alias"
version = 1

# Set the alias for the desired model version (e.g., version 1)
client = MlflowClient()
client.set_registered_model_alias(registered_name, alias, version)

# Load the model using the alias
loaded_model = mlflow.pyfunc.load_model(f"models:/{registered_name}@{alias}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ä½œæˆ
# MAGIC ãƒ¢ãƒ‡ãƒ«ãŒç™»éŒ²ã•ã‚ŒãŸã‚‰ã€APIã‚’ä½¿ç”¨ã—ã¦Databricks GPU Model Serving Endpointã‚’ä½œæˆã—ã€`bge-large-en`ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã—ã¦ã„ãã¾ã™ã€‚
# MAGIC
# MAGIC ä»¥ä¸‹ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã¯GPUãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°ãŒå¿…è¦ã§ã™ã€‚GPU ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€Databricks ãƒãƒ¼ãƒ ã«ãŠå•ã„åˆã‚ã›ã„ãŸã ãã‹ã€ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ— [ã“ã¡ã‚‰](https://docs.google.com/forms/d/1-GWIlfjlIaclqDz6BPODI2j1Xg4f4WbFvBXyebBpN-Y/edit) ã—ã¦ãã ã•ã„ã€‚

# COMMAND ----------

databricks_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)
token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)

# COMMAND ----------

# ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ä½œæˆã¾ãŸã¯æ›´æ–°
from datetime import timedelta
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedModelInput, ServedModelInputWorkloadSize, ServedModelInputWorkloadType
import mlflow
from mlflow import MlflowClient

# Define the registered model name and alias
registered_name = f"{model_uc_catalog}.{model_uc_schema}.{uc_model_name}"
alias = "latest_alias"
version = 1

# Set the alias for the desired model version (e.g., version 1)
client = MlflowClient()
client.set_registered_model_alias(registered_name, alias, version)

# Load the model using the alias
loaded_model = mlflow.pyfunc.load_model(f"models:/{registered_name}@{alias}")
serving_endpoint_name = endpoint_name

# Get the model version using the alias directly
model_uri = f"models:/{registered_name}@{alias}"
model_name = registered_name

# No need to get the latest model version using MLflow client
# latest_model_version = client.get_latest_versions(model_name, stages=["None"])[0].version

w = WorkspaceClient()
endpoint_config = EndpointCoreConfigInput(
    name=serving_endpoint_name,
    served_models=[
        ServedModelInput(
            model_name=model_name,
            model_version=version,  # Use the version directly
            workload_type=ServedModelInputWorkloadType.GPU_SMALL,
            workload_size=ServedModelInputWorkloadSize.SMALL,
            scale_to_zero_enabled=True
        )
    ]
)

existing_endpoint = next(
    (e for e in w.serving_endpoints.list() if e.name == serving_endpoint_name), None
)
serving_endpoint_url = f"{databricks_url}/ml/endpoints/{serving_endpoint_name}"
if existing_endpoint is None:
    print(f"Creating the endpoint {serving_endpoint_url}, this will take a few minutes to package and deploy the endpoint...")
    w.serving_endpoints.create_and_wait(name=serving_endpoint_name, config=endpoint_config, timeout=timedelta(minutes=60))
else:
    print(f"Updating the endpoint {serving_endpoint_url} to version {version}, this will take a few minutes to package and deploy the endpoint...")
    w.serving_endpoints.update_config_and_wait(served_models=endpoint_config.served_models, name=serving_endpoint_name, timeout=timedelta(minutes=60))
    
displayHTML(f'Your Model Endpoint Serving is now available. Open the <a href="/ml/endpoints/{serving_endpoint_name}">Model Serving Endpoint page</a> for more details.')

# COMMAND ----------

# MAGIC %md
# MAGIC ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æº–å‚™ãŒã§ããŸã‚‰ã€åŒã˜ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹MLflow Deployments SDKã§ç°¡å˜ã«ã‚¯ã‚¨ãƒªã§ãã¾ã™ã€‚

# COMMAND ----------

import mlflow.deployments

client = mlflow.deployments.get_deploy_client("databricks")

embeddings_response = client.predict(
    endpoint=endpoint_name,
    inputs={
        "inputs": ["ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™"]
    }
)
embeddings_response['predictions']
