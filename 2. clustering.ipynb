{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ab0a1b-7237-462f-8206-05e88b38972d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn numpy hdbscan bertopic langchain databricks-sdk[openai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721eef50-6740-4944-8303-c5d656428b96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "CATALOG = \"kohei_arai\"\n",
    "SCHEMA = \"demo\"\n",
    "WRITEBACK_TABLE = \"calls_log_silver_index_writeback_table\"\n",
    "PLOT_TABLE = \"calls_log_plot_table\"\n",
    "LABELS_TABLE = \"calls_log_labels_table\"\n",
    "GOLD_TABLE = \"calls_log_gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f714fa18-0149-4b83-b5a5-9ddf90ef760c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(f\"{CATALOG}.{SCHEMA}.{WRITEBACK_TABLE}\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f567e5-30f6-4ff6-b627-d896f3ed6af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_data(spark_df, vector_col=\"__db_content_summary_vector\"):\n",
    "    \"\"\"\n",
    "    Convert Spark DataFrame to numpy array for sklearn\n",
    "    \"\"\"\n",
    "    # Convert to pandas first (more efficient for spectral clustering)\n",
    "    pandas_df = spark_df.toPandas()\n",
    "    # Convert string representation of array to numpy array if needed\n",
    "    if isinstance(pandas_df[vector_col][0], str):\n",
    "        vectors = np.array([eval(v) for v in pandas_df[vector_col]])\n",
    "    else:\n",
    "        vectors = np.array(pandas_df[vector_col].tolist())\n",
    "    return vectors, pandas_df\n",
    "\n",
    "def find_optimal_clusters(X, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Find optimal number of clusters using silhouette score\n",
    "    \"\"\"\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        print(f\"Testing {n_clusters} clusters...\")\n",
    "        \n",
    "        # Initialize and fit Spectral Clustering\n",
    "        spectral = SpectralClustering(\n",
    "            n_clusters=n_clusters,\n",
    "            assign_labels='discretize',\n",
    "            random_state=42,\n",
    "            affinity='nearest_neighbors'  # Using KNN for sparse affinity matrix\n",
    "        )\n",
    "        cluster_labels = spectral.fit_predict(X)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        score = silhouette_score(X, cluster_labels)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"Silhouette score for {n_clusters} clusters: {score:.3f}\")\n",
    "    \n",
    "    optimal_clusters = np.argmax(silhouette_scores) + 2\n",
    "    return optimal_clusters\n",
    "\n",
    "def perform_spectral_clustering(X, n_clusters):\n",
    "    \"\"\"\n",
    "    Perform spectral clustering with optimal parameters\n",
    "    \"\"\"\n",
    "    spectral = SpectralClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        assign_labels='discretize',\n",
    "        random_state=24,\n",
    "        affinity='nearest_neighbors',\n",
    "        n_neighbors=10  # Adjust based on your data\n",
    "    )\n",
    "    return spectral.fit_predict(X)\n",
    "\n",
    "def create_spark_df_with_clusters(spark, original_df, cluster_labels):\n",
    "    \"\"\"\n",
    "    Create a new Spark DataFrame with cluster assignments\n",
    "    \"\"\"\n",
    "    # Convert cluster labels to Spark DataFrame\n",
    "    cluster_pd = pd.DataFrame(cluster_labels, columns=['cluster_id'])\n",
    "    cluster_spark = spark.createDataFrame(cluster_pd)\n",
    "    \n",
    "    # Add row index to both DataFrames\n",
    "    original_with_index = original_df.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "    cluster_with_index = cluster_spark.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "    \n",
    "    # Join the DataFrames\n",
    "    return original_with_index.join(\n",
    "        cluster_with_index,\n",
    "        on=\"row_idx\"\n",
    "    ).drop(\"row_idx\")\n",
    "\n",
    "# Main clustering pipeline\n",
    "def spectral_clustering_pipeline(spark_df, max_clusters=10, n_clusters=None):\n",
    "    \"\"\"\n",
    "    Main pipeline for spectral clustering\n",
    "    \"\"\"\n",
    "    print(\"Preparing data...\")\n",
    "    X, pandas_df = prepare_data(spark_df)\n",
    "    \n",
    "    if n_clusters is None:\n",
    "        print(\"Finding optimal number of clusters...\")\n",
    "        # n_clusters = find_optimal_clusters(X, max_clusters)\n",
    "        n_clusters = max_clusters\n",
    "        print(f\"Optimal number of clusters: {n_clusters}\")\n",
    "    \n",
    "    print(f\"Performing spectral clustering with {n_clusters} clusters...\")\n",
    "    cluster_labels = perform_spectral_clustering(X, n_clusters)\n",
    "    \n",
    "    # Create final DataFrame with cluster assignments\n",
    "    final_df = create_spark_df_with_clusters(spark, spark_df, cluster_labels)\n",
    "    \n",
    "    # Show cluster distribution\n",
    "    print(\"\\nCluster distribution:\")\n",
    "    final_df.groupBy(\"cluster_id\").count().orderBy(\"cluster_id\").show()\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Run the pipeline\n",
    "clustered_df = spectral_clustering_pipeline(\n",
    "    df,\n",
    "    max_clusters=5  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "display(clustered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4b6056-b5fd-4238-8ff6-8f929f72e679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "vectors = np.array(clustered_df.select('__db_content_summary_vector').toPandas()['__db_content_summary_vector'].tolist())\n",
    "clusters = np.array(clustered_df.select('cluster_id').toPandas()['cluster_id'].tolist())\n",
    "texts = clustered_df.select('content_summary').toPandas()['content_summary'].tolist()\n",
    "\n",
    "# Perform t-SNE\n",
    "print(\"Performing t-SNE dimensionality reduction...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "embeddings_2d = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    'x': embeddings_2d[:, 0],\n",
    "    'y': embeddings_2d[:, 1],\n",
    "    'cluster': clusters.astype(str),  # Convert to string for better legends\n",
    "    'text': texts\n",
    "})\n",
    "\n",
    "# Map cluster IDs to colors\n",
    "unique_clusters = plot_df['cluster'].unique()\n",
    "color_map = {cluster: px.colors.qualitative.Set3[i % len(px.colors.qualitative.Set3)] for i, cluster in enumerate(unique_clusters)}\n",
    "plot_df['color'] = plot_df['cluster'].map(color_map)\n",
    "\n",
    "# Create subplots: main scatter plot and cluster distribution\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    column_widths=[0.7, 0.3],\n",
    "    specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}]],\n",
    "    subplot_titles=('Cluster Visualization (t-SNE)', 'Cluster Distribution')\n",
    ")\n",
    "\n",
    "# Add scatter plot\n",
    "scatter = go.Scatter(\n",
    "    x=plot_df['x'],\n",
    "    y=plot_df['y'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=8,\n",
    "        color=plot_df['color'],\n",
    "        showscale=False\n",
    "    ),\n",
    "    text=plot_df['text'],\n",
    "    hovertemplate=\"<b>Cluster:</b> %{marker.color}<br>\" +\n",
    "                  \"<b>Text:</b> %{text}<br>\" +\n",
    "                  \"<extra></extra>\",\n",
    "    showlegend=True,\n",
    "    name='Clusters'\n",
    ")\n",
    "fig.add_trace(scatter, row=1, col=1)\n",
    "\n",
    "# Add cluster distribution bar chart\n",
    "cluster_counts = plot_df['cluster'].value_counts().sort_index()\n",
    "bar = go.Bar(\n",
    "    x=cluster_counts.index,\n",
    "    y=cluster_counts.values,\n",
    "    name='Cluster Size',\n",
    "    marker_color=px.colors.qualitative.Set3[:len(cluster_counts)],\n",
    "    hovertemplate=\"<b>Cluster:</b> %{x}<br>\" +\n",
    "                  \"<b>Count:</b> %{y}<br>\" +\n",
    "                  \"<extra></extra>\"\n",
    ")\n",
    "fig.add_trace(bar, row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Cluster Analysis Dashboard\",\n",
    "    title_x=0.5,\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    showlegend=True,\n",
    "    template='plotly_white',\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(title_text=\"t-SNE dimension 1\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"t-SNE dimension 2\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Cluster\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Number of items\", row=1, col=2)\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n",
    "\n",
    "# Print cluster statistics\n",
    "print(\"\\nDetailed Cluster Statistics:\")\n",
    "cluster_stats = pd.DataFrame({\n",
    "    'Cluster': cluster_counts.index,\n",
    "    'Count': cluster_counts.values,\n",
    "    'Percentage': (cluster_counts.values / len(plot_df) * 100).round(2)\n",
    "})\n",
    "print(cluster_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c6ca3f-47d9-44fc-b049-d04504fa2305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e749bca-333c-41e5-9dfb-294d8c6ea45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(plot_df).write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.{PLOT_TABLE}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2. clustering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
